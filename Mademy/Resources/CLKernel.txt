
float ActivationFunction(float x, int functionId)
{
	switch(functionId)
	{
		case 1:
			return 1.0f/(1.0f + exp(-x));
		case 0:
		default:
			return x;
	}
}


__kernel void calcSingleLayer(__global const float* weightMx, __global const float* biases, __global const float* prevActivation, __global const int* config, __global float* output) 
{
    const int rowCount = config[0]; //number of neurons
    const int colCount = config[1]; //weights-per-neurons
    const int activationFunctionId = config[2]; 

    const int rowId = get_global_id(0);
 
    if (rowId >= rowCount)
        return;

    float acc = 0;
    for(int i = 0; i < colCount; ++i)
        acc += weightMx[(rowId * colCount) + i] * prevActivation[i];
    acc += biases[rowId];

    acc = ActivationFunction(acc, activationFunctionId);

    output[rowId] = acc;
}

int GetWeightAndBiasOffset(int layerCount, __global const int* layerSizes)
{
    int offset = 0;
    for(int i = 0; i < layerCount; ++i){
        offset += layerSizes[i] * layerSizes[i+1] + layerSizes[i+1]; //prevWeights * neuroncCount (weights) + neuronCount (biases)
    }
    return offset;
}

int GetActivationLayerOffset(int layerCount, __global const int* layerSizes)
{
    int offset = 0;
    for(int i = 0; i < layerCount; ++i){
        offset += layerSizes[i+1]; //neuron count
    }
    return offset;
}

__kernel void trainingForwardPass(__global const int* config, __global float* activationsAndZValues, __global const float* inputValues, __global const float* weightsAndBiases) 
{
    const int layerId = config[0]; //The layer's index to be calculated
    const int layerCount = config[1]; //number of layers in the network
    const int numTrainingSamples = config[2]; //number of layers in the network
    const int activationFunctionId = config[3]; //which activation function to use
    const int totalActivationCount = config[5]; //How many activations are there in total in the network
    const int neuronsInLayer = config[7 + layerId]; //number of neurons in the current layer
    const int weightsPerNeuron = config[6 + layerId]; //number of weights per neurons on the current layer
    __global const int* layerNeuronCountBegin = config+6;

    const int neuronId = get_global_id(0);
    const int trainingSample = get_global_id(1);

    if (trainingSample >= numTrainingSamples || neuronId >= neuronsInLayer)
    {
        return;
    }

    const int activationTableOffset = totalActivationCount * trainingSample;

    const int weightOffset = GetWeightAndBiasOffset(layerId, layerNeuronCountBegin);
    const int biasOffset = weightOffset + neuronsInLayer * weightsPerNeuron;
    const int activationOffset = activationTableOffset + GetActivationLayerOffset(layerId, layerNeuronCountBegin);
    const int zValueOffset = activationOffset + (numTrainingSamples * totalActivationCount); //shift by table size
    const int prevActivationListOffset = (activationTableOffset + GetActivationLayerOffset(layerId - 1, layerNeuronCountBegin));

    __global const float* prevActivations = layerId == 0 ? (inputValues + weightsPerNeuron * trainingSample) : (activationsAndZValues + prevActivationListOffset );

    float acc = 0;
    for(int i = 0; i < neuronsInLayer; ++i)
        acc += weightsAndBiases[weightOffset + (neuronId * neuronsInLayer) + i] * prevActivations[i];
    acc += weightsAndBiases[biasOffset + neuronId];

    activationsAndZValues[zValueOffset+neuronId] = acc;
    activationsAndZValues[activationOffset+neuronId] = ActivationFunction(acc, activationFunctionId);
}

__kernel void trainingOutputLayer(__global const int* configParams, __global const float* activationsAndZValues, __global float* delta_k_vector, __global float* gradient) 
{
}

__kernel void trainingHiddenLayer(__global const int* configParams, __global const float* activationsAndZValues, __global float* delta_k_vector, __global float* gradient) 
{
}